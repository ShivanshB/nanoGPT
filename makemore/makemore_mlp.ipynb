{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec560d56",
   "metadata": {},
   "source": [
    "# Makemore MLP\n",
    "\n",
    "### Using an MLP framework with an embedding space lookup table and variable-length letter context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8a1a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c56f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5d5ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(''.join(words)))\n",
    "\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "\n",
    "itos = {i:s for s,i in stoi.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4692d7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 3\n",
    "context = [0] * context_length\n",
    "\n",
    "X, Y = [], []\n",
    "random.shuffle(words) # shuffle the dataset\n",
    "\n",
    "for w in words:    \n",
    "    for char in w + '.':\n",
    "        X.append(context)\n",
    "        Y.append(stoi[char])  \n",
    "        context = context[1:] + [stoi[char]]\n",
    "        \n",
    "X, Y = torch.tensor(X), torch.tensor(Y)\n",
    "\n",
    "n1, n2 = int(len(X)*0.8), int(len(X)*0.9)\n",
    "Xtr, Ytr = X[:n1], Y[:n1]\n",
    "Xval, Yval = X[n1:n2], Y[n1:n2]\n",
    "Xtest, Ytest = X[n2:], Y[n2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6cad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae26414",
   "metadata": {},
   "source": [
    "### Embedding Space\n",
    "We will start by using an embedding space of length two vectors for each letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1760e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "C = torch.randn((27, 2), requires_grad=True)\n",
    "W1 = torch.randn((6, 100), requires_grad=True)\n",
    "b1 = torch.randn(100, requires_grad=True)\n",
    "W2 = torch.randn((100, 27), requires_grad=True)\n",
    "b2 = torch.randn(27, requires_grad=True)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "n = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836ea316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "emb = C[X].view(-1, 6)\n",
    "h = (emb @ W1 + b1).tanh()\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "# calculating loss\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2b2819",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9c29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "NUM_ITERS = 100\n",
    "lr = 1\n",
    "\n",
    "for i in range(NUM_ITERS):\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X].view(-1, 6)\n",
    "    h = (emb @ W1 + b1).tanh()\n",
    "    logits = h @ W2 + b2\n",
    "\n",
    "    # calculating loss\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "    \n",
    "    # clearing old gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    # backprop & update\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd8eaab",
   "metadata": {},
   "source": [
    "Notice that this is relatively slow, since we are doing a forward pass with all 200k+ data points and only then backpropagating with their gradients. In theory, we should be able to use exactly one data point, then calculate the loss and backpropagate with that one point to adjust our params. This would allow us to obtain a lower loss (and possibly a less noisy gradient step) when we use our next data point. However, notice this trade off. Using the gradient from one data point is too noisy and while using all of the data points is too slow/compute intensive. We would like an accurate (not-noisy) gradient, while also being able to take our gradient steps. We can compromise by using SGD on a minibatch of data, using (usually some multiple of 2) data points and using this (now significantly less noisy signal) to take our gradient step while also being multiple orders of magnitude faster. Turns out that in this way, taking more (slightly less accurate steps) gets us further than taking fewer/slower, precisely calculated steps. (SGD as a metaphor for life and perfectionism?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec1d96",
   "metadata": {},
   "source": [
    "### Minibatch Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85020a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset params to retrain\n",
    "C = torch.randn((27, 2), requires_grad=True)\n",
    "W1 = torch.randn((6, 100), requires_grad=True)\n",
    "b1 = torch.randn(100, requires_grad=True)\n",
    "W2 = torch.randn((100, 27), requires_grad=True)\n",
    "b2 = torch.randn(27, requires_grad=True)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "n = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37122bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch training loop\n",
    "NUM_ITERS = 10000\n",
    "lr = .01\n",
    "\n",
    "for i in range(NUM_ITERS):\n",
    "    \n",
    "    # minibatch indices\n",
    "    idxs = torch.randint(0, n, (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X[idxs]].view(-1, 6)\n",
    "    h = (emb @ W1 + b1).tanh()\n",
    "    logits = h @ W2 + b2\n",
    "\n",
    "    # calculating loss\n",
    "    loss = F.cross_entropy(logits, Y[idxs])\n",
    "    \n",
    "    # clearing old gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    # backprop & update\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e13427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss on entire dataset\n",
    "emb = C[X].view(-1, 6)\n",
    "h = (emb @ W1 + b1).tanh()\n",
    "logits = h @ W2 + b2\n",
    "\n",
    "# calculating loss\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd96f5b8",
   "metadata": {},
   "source": [
    "We're going to do a learning-rate search. First we will do some initial loss convergence testing to determine a reasonable range. We're assuming the distribtion for a good lr is unimodal (relatively speaking, at least)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715ba60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_initial_test = torch.pow(10, torch.arange(-5, 5, 1).float())\n",
    "lr_initial_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58eaa801",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_test(lr):\n",
    "    # minibatch training loop\n",
    "    NUM_ITERS = 1000\n",
    "    losses = []\n",
    "    \n",
    "    # reset params to retrain\n",
    "    C = torch.randn((27, 2), requires_grad=True)\n",
    "    W1 = torch.randn((6, 100), requires_grad=True)\n",
    "    b1 = torch.randn(100, requires_grad=True)\n",
    "    W2 = torch.randn((100, 27), requires_grad=True)\n",
    "    b2 = torch.randn(27, requires_grad=True)\n",
    "    parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "    n = len(X)\n",
    "\n",
    "    for i in range(NUM_ITERS):\n",
    "\n",
    "        # minibatch indices\n",
    "        idxs = torch.randint(0, n, (32,))\n",
    "\n",
    "        # forward pass\n",
    "        emb = C[X[idxs]].view(-1, 6)\n",
    "        h = (emb @ W1 + b1).tanh()\n",
    "        logits = h @ W2 + b2\n",
    "\n",
    "        # calculating loss\n",
    "        loss = F.cross_entropy(logits, Y[idxs])\n",
    "        losses.append(loss)\n",
    "\n",
    "        # clearing old gradients\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "\n",
    "        # backprop & update\n",
    "        loss.backward()\n",
    "\n",
    "        for p in parameters:\n",
    "            p.data -= lr * p.grad\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb26711",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "for lr in lr_initial_test:\n",
    "    losses.append(lr_test(lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a73cfe7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range():\n",
    "        plt.plot(np.arange(1000), losses[i], label=f\"lr={lr_initial_test[i]}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f05a4",
   "metadata": {},
   "source": [
    "Since the highest and lowest learning rates don't converge, we remove them from the plot so that we can scale down and see what is happening with the relevant values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb76de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(1, 7):\n",
    "        plt.plot(np.arange(1000), losses[i], label=f\"lr={lr_initial_test[i]}\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba88115",
   "metadata": {},
   "source": [
    "Here, we see that only really the $\\text{learning rates} \\in [0.001, 1]$ converge and that the learning rate of 0.2 seems to do so in the most stable fashion to the lowest value. We could further refine our search and see how the lossses converge on a more dense region of our new interval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5151ac1f",
   "metadata": {},
   "source": [
    "#### Note on the train, val, test split: \n",
    "training data --> train model parameters \\\n",
    "val/dev data  --> train hyperparameters \\\n",
    "test data     --> test model (very infrequently)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e825e9fd",
   "metadata": {},
   "source": [
    "#### On Model Size and Under/Overfitting\n",
    "\n",
    "Often, if the loss on the training set and val set is the same or if the loss on the val set is slightly lower than the training let, we have underfit and we can expect to see an improvement in performance (decrease in loss), if we increase the size of the model (more hidden layers, larger hidden layers, etc). If we do this and we don't see any improvement in performance then we could either (1) not be optimizing our larger network correctly, or (2) there may be a bottleneck in another part of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdc25be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize dimensions 0 and 1 of the embedding matrix C for all characters\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.scatter(C[:,0].data, C[:,1].data, s=200)\n",
    "for i in range(C.shape[0]):\n",
    "    plt.text(C[i,0].item(), C[i,1].item(), itos[i], ha=\"center\", va=\"center\", color='white')\n",
    "plt.grid('minor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ccf52",
   "metadata": {},
   "source": [
    "Notice that these embeddings display some notable structure. All of the vowels are grouped on the middle left hand side. Further, the '.' and 'q' have been embedded as unique, which is qualitatively true in my understanding of English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d806511c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset params to retrain\n",
    "C = torch.randn((27, 15), requires_grad=True)\n",
    "W1 = torch.randn((45, 250), requires_grad=True)\n",
    "b1 = torch.randn(250, requires_grad=True)\n",
    "W2 = torch.randn((250, 250), requires_grad=True)\n",
    "b2 = torch.randn(250 , requires_grad=True)\n",
    "W3 = torch.randn((250, 27), requires_grad=True)\n",
    "b3 = torch.randn(27, requires_grad=True)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, W3, b3]\n",
    "\n",
    "n = len(Xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1323b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch training loop\n",
    "NUM_ITERS = 300000\n",
    "\n",
    "for i in range(NUM_ITERS):\n",
    "    \n",
    "    # minibatch indices\n",
    "    idxs = torch.randint(0, n, (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xtr[idxs]].view(-1, 45)\n",
    "    h = (emb @ W1 + b1).tanh()\n",
    "    logits = ((h @ W2 + b2).tanh()) @ W3 + b3\n",
    "\n",
    "    # calculating loss\n",
    "    loss = F.cross_entropy(logits, Ytr[idxs])\n",
    "    \n",
    "    # clearing old gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    # backprop & update\n",
    "    loss.backward()\n",
    "    \n",
    "    lr = 0.1 if i < 150000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5f4d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss on val set\n",
    "emb = C[Xval].view(-1, 45)\n",
    "h = (emb @ W1 + b1).tanh()\n",
    "logits = ((h @ W2 + b2).tanh()) @ W3 + b3\n",
    "\n",
    "# calculating loss\n",
    "loss = F.cross_entropy(logits, Yval)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682b8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 10\n",
    "context = [0] * context_length\n",
    "\n",
    "for _ in range(NUM_SAMPLES):\n",
    "    sample = []\n",
    "    \n",
    "    while True:\n",
    "        emb = C[context].view(1, 45)\n",
    "        h = (emb @ W1 + b1).tanh()\n",
    "        logits = (((h @ W2) + b2).tanh() @ W3) + b3\n",
    "        probs = F.softmax(logits, dim = 1)\n",
    "        \n",
    "        idx = torch.multinomial(probs, num_samples=1).item()\n",
    "        sample.append(idx)\n",
    "        context = context[1:] + [idx]\n",
    "        \n",
    "        if idx == 0:\n",
    "            break\n",
    "        \n",
    "    print(''.join(itos[idx] for idx in sample))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d865f2a5",
   "metadata": {},
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ded983",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    plt.plot(np.arange(len(losses[5])), losses[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103cd839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.log(1/27.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc28e612",
   "metadata": {},
   "source": [
    "Note on Initialization and Loss Graphs:\n",
    "\n",
    "Notice the hockey stick appearance of the loss curve above. This tells us that the initialization of our parameters in our network is not very good (since it gives such a high loss). Lets think about what it means to have a good loss in this problem and what we should expect from our model before training. Before training, our model should have no information about which characters are more likely than others, and should thus output a roughly uniform distribution over our alphabet. Doing some quick math, this tells us that we should expect a loss of around $-log(\\frac{1}{27}) \\approx 3.2958 $. Notice in the graph above though, that we have a loss of more than 17.5. This means that our model is initially outputting probabilities with a high variance (very bold/confident in its predictions), while being wrong most of the time. Notice that we obtain our probabilites from a softmax, so in order to get a nearly uniform distribution, we would need nearly uniform logit values. This can be done by multiplying our bias and weight matrices (all or just the last couple) by some small constants to squish their values to around zero. In theory, we could also just set one pair of weight and bias vectors to exactly all zeros and this should give us a perfectly uniform distribution. However, we must be careful when doing this, since it can cause unwanted symmetry in the network, as the gradients will propogate backwards exactly the same way throughout the entire network. In this way, we say that some variance in the weights is necessary to \"break the symmetry of the network\". \n",
    "\n",
    "I will now demonstrate some of the above ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset params to retrain\n",
    "C = torch.randn((27, 2), requires_grad=True)\n",
    "W1 = torch.randn((6, 100), requires_grad=True)\n",
    "b1 = torch.randn(100, requires_grad=True)\n",
    "W2 = torch.randn((100, 27), requires_grad=True)\n",
    "b2 = torch.randn(27, requires_grad=True)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "n = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805fc55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch training loop\n",
    "NUM_ITERS = 1000\n",
    "lr = .01\n",
    "\n",
    "for i in range(NUM_ITERS):\n",
    "    \n",
    "    # minibatch indices\n",
    "    idxs = torch.randint(0, n, (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X[idxs]].view(-1, 6)\n",
    "    h = (emb @ W1 + b1).tanh()\n",
    "    logits = h @ W2 + b2\n",
    "\n",
    "    # calculating loss\n",
    "    loss = F.cross_entropy(logits, Y[idxs])\n",
    "    \n",
    "    # clearing old gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    # backprop & update\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "    print(loss.item())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e9954e",
   "metadata": {},
   "source": [
    "Here, notice that the initial loss is much higher than the expected 3.29... \\\n",
    "Now, if we set W2, b2 to all zeros, notice what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e562ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset params to retrain\n",
    "C = torch.randn((27, 2), requires_grad=True)\n",
    "W1 = torch.randn((6, 100), requires_grad=True)\n",
    "b1 = torch.randn(100, requires_grad=True)\n",
    "W2 = torch.zeros((100, 27), requires_grad=True)\n",
    "b2 = torch.zeros(27, requires_grad=True)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "n = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcc6803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch training loop\n",
    "NUM_ITERS = 1000\n",
    "lr = .01\n",
    "\n",
    "for i in range(NUM_ITERS):\n",
    "    \n",
    "    # minibatch indices\n",
    "    idxs = torch.randint(0, n, (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X[idxs]].view(-1, 6)\n",
    "    h = (emb @ W1 + b1).tanh()\n",
    "    logits = h @ W2 + b2\n",
    "\n",
    "    # calculating loss\n",
    "    loss = F.cross_entropy(logits, Y[idxs])\n",
    "    \n",
    "    # clearing old gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    # backprop & update\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "    print(loss.item())\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6b280",
   "metadata": {},
   "source": [
    "Now, we get a loss of exactly 3.29, as expected from a uniform distribution.\n",
    "Now, let's see what happens if we set all of our weights and biases to all zeros and try to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ec190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset params to retrain\n",
    "C = torch.zeros((27, 2), requires_grad=True)\n",
    "W1 = torch.zeros((6, 100), requires_grad=True)\n",
    "b1 = torch.zeros(100, requires_grad=True)\n",
    "W2 = torch.zeros((100, 27), requires_grad=True)\n",
    "b2 = torch.zeros(27, requires_grad=True)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "\n",
    "n = len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f10870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch training loop\n",
    "NUM_ITERS = 10000\n",
    "lr = .01\n",
    "\n",
    "for i in range(NUM_ITERS):\n",
    "    \n",
    "    # minibatch indices\n",
    "    idxs = torch.randint(0, n, (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X[idxs]].view(-1, 6)\n",
    "    h = (emb @ W1 + b1).tanh()\n",
    "    logits = h @ W2 + b2\n",
    "\n",
    "    # calculating loss\n",
    "    loss = F.cross_entropy(logits, Y[idxs])\n",
    "    \n",
    "    # clearing old gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    # backprop & update\n",
    "    loss.backward()\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6efffc",
   "metadata": {},
   "source": [
    "Though this initialization does converge (because of the presence of the tanh which allows the gradient to be nonzero), it may not in other cases and so all zero initialization is not recommended. We will now use a more stable initialization of small values that are not actually zero and see how this affects our achieved loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9804b77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset params to retrain\n",
    "C = torch.randn((27, 15), requires_grad=True)\n",
    "W1 = torch.randn((45, 250), requires_grad=True)\n",
    "b1 = torch.randn(250, requires_grad=True)\n",
    "\n",
    "W2 = torch.randn((250, 250)) * 0.01\n",
    "W2.requires_grad = True\n",
    "\n",
    "b2 = torch.randn(250 , requires_grad=True)\n",
    "W3 = torch.randn((250, 27), requires_grad=True)\n",
    "b3 = torch.randn(27, requires_grad=True)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, W3, b3]\n",
    "\n",
    "n = len(Xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d068113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch training loop\n",
    "NUM_ITERS = 300000\n",
    "\n",
    "for i in range(NUM_ITERS):\n",
    "    \n",
    "    # minibatch indices\n",
    "    idxs = torch.randint(0, n, (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xtr[idxs]].view(-1, 45)\n",
    "    h = (emb @ W1 + b1).tanh()\n",
    "    logits = ((h @ W2 + b2).tanh()) @ W3 + b3\n",
    "\n",
    "    # calculating loss\n",
    "    loss = F.cross_entropy(logits, Ytr[idxs])\n",
    "    \n",
    "    # clearing old gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    # backprop & update\n",
    "    loss.backward()\n",
    "    \n",
    "    lr = 0.1 if i < 150000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87474051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss on val set\n",
    "emb = C[Xval].view(-1, 45)\n",
    "h = (emb @ W1 + b1).tanh()\n",
    "logits = ((h @ W2 + b2).tanh()) @ W3 + b3\n",
    "\n",
    "# calculating loss\n",
    "loss = F.cross_entropy(logits, Yval)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4774edc5",
   "metadata": {},
   "source": [
    "Notice that we achieve a slightly lower validation loss here since we get to spend more time training the actual difficult part of the model, instead of just spending the first couple iterations squishing the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f89797f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset params to retrain\n",
    "C = torch.randn((27, 15), requires_grad=True)\n",
    "W1 = torch.randn((45, 250), requires_grad=True)\n",
    "b1 = torch.randn(250, requires_grad=True)\n",
    "\n",
    "W2 = torch.randn((250, 27)) * 0.01\n",
    "W2.requires_grad = True\n",
    "b2 = torch.randn(27 , requires_grad=True)\n",
    "\n",
    "W3 = torch.randn((45, 27), requires_grad=True)\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, W3]\n",
    "\n",
    "n = len(Xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bd31de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minibatch training loop\n",
    "NUM_ITERS = 300000\n",
    "\n",
    "for i in range(NUM_ITERS):\n",
    "    \n",
    "    # minibatch indices\n",
    "    idxs = torch.randint(0, n, (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xtr[idxs]].view(-1, 45)\n",
    "    h = (emb @ W1 + b1).tanh()\n",
    "    logits = ((h @ W2 + b2).tanh()) + emb @ W3\n",
    "\n",
    "\n",
    "    # calculating loss\n",
    "    loss = F.cross_entropy(logits, Ytr[idxs])\n",
    "    \n",
    "    # clearing old gradients\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    \n",
    "    # backprop & update\n",
    "    loss.backward()\n",
    "    \n",
    "    lr = 0.1 if i < 150000 else 0.01\n",
    "    for p in parameters:\n",
    "        p.data -= lr * p.grad\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24089d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss on val set\n",
    "emb = C[Xval].view(-1, 45)\n",
    "h = (emb @ W1 + b1).tanh()\n",
    "logits = ((h @ W2 + b2).tanh()) + emb @ W3\n",
    "\n",
    "# calculating loss\n",
    "loss = F.cross_entropy(logits, Yval)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d221c0",
   "metadata": {},
   "source": [
    "Though this did not decrease the loss in any significant way, we have addded a skip connection. It likely didn't help much since this model is not very deep to begin with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4699d70e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
